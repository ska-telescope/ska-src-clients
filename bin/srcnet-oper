#!/usr/bin/env python

import argparse
import datetime
import json
import logging
import os
import yaml
from pathlib import Path

import plotly.graph_objects as go
import pprint
from box import Box
from prettytable import PrettyTable

from ska_src_clients.api import DataAPI, MetadataAPI, ServicesAPI, SiteAPI
from ska_src_clients.common.utility import plot_scatter_world_map, url_to_parts
from ska_src_clients.session.oidc import OIDCSession


def make_parsers(return_all_parsers=False):
    def add_api_commands_to_subparsers(subparsers):
        api_parser = subparsers.add_parser('api', help="API operations")
        api_parser_subparsers = api_parser.add_subparsers(help="API operations", dest='subcommand')

        # api health
        api_health_parser = api_parser_subparsers.add_parser("health", help="Get the API health")
        api_health_parser.add_argument('service', help="The service name")

        # api ls
        api_parser_subparsers.add_parser("ls", help="List APIs")

        # api ping
        api_health_parser = api_parser_subparsers.add_parser("ping", help="Ping the API")
        api_health_parser.add_argument('service', help="The service name")

        return (api_parser,)

    def add_config_commands_to_subparsers(subparsers):
        config_parser = subparsers.add_parser("config", help="Configuration file operations")
        config_parser_subparsers = config_parser.add_subparsers(help="Configuration file operations", dest='subcommand')

        # config get
        config_get_parser = config_parser_subparsers.add_parser("get", help="Get a configuration file")
        config_get_parser.add_argument('--section', help="Configuration file section to return", required=True)

        return (config_parser,)

    def add_data_commands_to_subparsers(subparsers):
        data_parser = subparsers.add_parser("data", help="Data operations")
        data_parser_subparsers = data_parser.add_subparsers(help="Data operations", dest='subcommand')

        # data download
        data_download_parser = data_parser_subparsers.add_parser(
            "download", help="Download a replica of a data identifier.")
        data_download_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_download_parser.add_argument('--name', default='*', help="The data identifier name (wildcards allowed)",
                                    required=True)
        data_download_parser.add_argument('--sort', default="nearest_by_ip", help="The sorting algorithm to use "
                                                                                   "(random || nearest_by_ip)")
        data_download_parser.add_argument('--ip_address', default="", help="The ip address to geolocate the nearest "
                                                                           "replica to. Leave blank to use the "
                                                                           "requesting client ip "
                                                                           "(sort == nearest_by_ip only)")
        data_download_parser.add_argument('--output', help="The output filename")

        # data locate
        data_locate_parser = data_parser_subparsers.add_parser("locate", help="Locate replicas of data identifiers.")
        data_locate_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_locate_parser.add_argument('--name', default='*', help="The data identifier name (wildcards allowed)",
                                          required=True)
        data_locate_parser.add_argument('--sort', default="nearest_by_ip", help="The sorting algorithm to use "
                                                                                "(random || nearest_by_ip)")
        data_locate_parser.add_argument('--ip_address', default="", help="The ip address to geolocate the nearest "
                                                                         "requesting client ip "
                                                                         "(sort == nearest_by_ip only)")
        data_locate_parser.add_argument('--plot', action="store_true", help="Plot?")

        # data ls
        data_ls_parser = data_parser_subparsers.add_parser("ls", help="List data identifiers in a namespace")
        data_ls_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_ls_parser.add_argument('--name', default='*', help="The data identifier name (wildcards allowed)",
                                    required=True)
        data_ls_parser.add_argument('--detail', action="store_true", help="More detail?")
        data_ls_parser.add_argument('--filters', default=None, help="Filters (Rucio only)")
        data_ls_parser.add_argument('--limit', default=100, help="Number of identifiers to return in result")

        # data namespace
        data_namespace_parser = data_parser_subparsers.add_parser("namespace", help="Data namespace operations")
        data_namespace_parser_subparsers = data_namespace_parser.add_subparsers(help="Data namespace operations",
                                                                                dest='subsubcommand')

        # data namespace ls
        data_namespace_parser_subparsers.add_parser("ls", help="List namespaces")

        # data rules
        data_rules_parser = data_parser_subparsers.add_parser("rules", help="Data rules operations")
        data_rules_parser_subparsers = data_rules_parser.add_subparsers(help="Data rules operations",
                                                                        dest='subsubcommand')

        # data rules ls
        data_rules_ls_parser = data_rules_parser_subparsers.add_parser("ls", help="List rules for a namespace")
        data_rules_ls_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_rules_ls_parser.add_argument('--datetime_from', default="now-1h", help="Human readable datetime from")
        data_rules_ls_parser.add_argument('--datetime_to', default="now", help="Human readable datetime to")
        data_rules_ls_parser.add_argument('--limit', default=1000, help="Human readable datetime to")
        data_rules_ls_parser.add_argument('--plot', action="store_true", help="Plot?")

        # data rules did
        data_rules_did_parser = data_rules_parser_subparsers.add_parser("did", help="List rules for a data identifier")
        data_rules_did_parser.add_argument('--namespace', help="The data identifier's namespace", required=True)
        data_rules_did_parser.add_argument('--name', help="The data identifier's namespace", required=True)
        data_rules_did_parser.add_argument('--plot', action="store_true", help="Plot?")

        # data upload
        data_upload_parser = data_parser_subparsers.add_parser("upload", help="Data upload operations")
        data_upload_parser_subparsers = data_upload_parser.add_subparsers(help="Data upload operations",
                                                                          dest='subsubcommand')

        # data upload ingest
        data_upload_ingest_parser = data_upload_parser_subparsers.add_parser("ingest",
                                                                             help="Upload data to ingest area")
        data_upload_ingest_parser.add_argument('--path', help="Local path to data directory to be uploaded",
                                               required=True)
        data_upload_ingest_parser.add_argument('--ingest-service-id', help="The ingest service id", required=True)
        data_upload_ingest_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_upload_ingest_parser.add_argument('--extra-metadata', help="Extra metadata to apply to each file (JSON)",
                                               default='{}')
        data_upload_ingest_parser.add_argument('--metadata-suffix', help="The expected metadata suffix",
                                               default='.meta')

        return data_parser, data_namespace_parser, data_rules_parser, data_upload_parser

    def add_metadata_commands_to_subparsers(subparsers):
        metadata_parser = subparsers.add_parser("metadata", help="Metadata operations")
        metadata_parser_subparsers = metadata_parser.add_subparsers(help="Metadata operations", dest='subcommand')

        # metadata get
        metadata_get_parser = metadata_parser_subparsers.add_parser("get", help="Get metadata for a data identifier")
        metadata_get_parser.add_argument('--namespace', help="The data identifier's namespace", required=True)
        metadata_get_parser.add_argument('--name', help="The data identifier's name", required=True)
        metadata_get_parser.add_argument('--store', default="file", help="The metadata store to use")
        metadata_get_parser.add_argument('--showempty', action="store_true", help="Show empty keys?")

        # metadata set
        metadata_set_parser = metadata_parser_subparsers.add_parser("set", help="Set metadata for a data identifier")
        metadata_set_parser.add_argument('--namespace', help="The data identifier's namespace", required=True)
        metadata_set_parser.add_argument('--name', help="The data identifier's name", required=True)
        metadata_set_parser.add_argument('--metadata', help="Metadata to set (JSON)", required=True)

        return (metadata_parser,)

    def add_site_commands_to_subparsers(subparsers):
        site_parser = subparsers.add_parser("site", help="Site operations")
        site_parser_subparsers = site_parser.add_subparsers(help="Site operations", dest='subcommand')

        # site add
        site_parser_subparsers.add_parser("add", help="Add a new site")

        # site edit
        site_edit_parser = site_parser_subparsers.add_parser("edit", help="Edit an existing site")
        site_edit_parser.add_argument('--site', help="Name of the site to edit", required=True)

        # site ls
        site_parser_subparsers.add_parser("ls", help="List sites")

        # site compute
        site_compute_parser = site_parser_subparsers.add_parser("compute", help="Site compute operations")
        site_compute_parser_subparsers = site_compute_parser.add_subparsers(help="Site compute operations",
                                                                            dest='subsubcommand')

        # site compute get
        site_compute_get_parser = site_compute_parser_subparsers.add_parser(
            "get", help="Get description of a compute element")
        site_compute_get_parser.add_argument('--id', help="ID of the compute element to get", required=True)

        # site compute ls
        site_compute_parser_subparsers.add_parser("ls", help="List compute elements")

        # site service
        site_service_parser = site_parser_subparsers.add_parser("service", help="Site service operations")
        site_service_parser_subparsers = site_service_parser.add_subparsers(help="Site service operations",
                                                                            dest='subsubcommand')

        # site service get
        site_service_get_parser = site_service_parser_subparsers.add_parser(
            "get", help="Get description of a service")
        site_service_get_parser.add_argument('--id', help="ID of the service to get", required=True)

        # site service ls
        site_service_ls_parser = site_service_parser_subparsers.add_parser("ls", help="List services")
        site_service_ls_parser.add_argument('--type', help="Filter for type of service (accepts wildcards)")

        # site service types
        site_service_parser_subparsers.add_parser("types", help="List service types")

        # site storage
        site_storage_parser = site_parser_subparsers.add_parser("storage", help="Site storage operations")
        site_storage_parser_subparsers = site_storage_parser.add_subparsers(help="Site storage operations",
                                                                            dest='subsubcommand')

        # site storage get
        site_storage_get_parser = site_storage_parser_subparsers.add_parser(
            "get", help="Get description of a storage resource")
        site_storage_get_parser.add_argument('--id', help="ID of the storage resource to get", required=True)

        # site storage ls
        site_storage_parser_subparsers.add_parser("ls", help="List storages")

        # site storage area
        site_storage_area_parser = site_parser_subparsers.add_parser("storage-area",
                                                                     help="Site storage area operations")
        site_storage_area_parser_subparsers = site_storage_area_parser.add_subparsers(
            help="Site storage area operations", dest='subsubcommand')

        # site storage area get
        site_storage_area_get_parser = site_storage_area_parser_subparsers.add_parser(
            "get", help="Get description of a storage area resource")
        site_storage_area_get_parser.add_argument('--id', help="ID of the storage area_resource to get", required=True)

        # site storage area ls
        site_storage_area_parser_subparsers.add_parser("ls", help="List storage areas")

        return site_parser, site_compute_parser, site_service_parser, site_storage_parser, site_storage_area_parser

    def add_token_commands_to_subparsers(subparsers):
        token_parser = subparsers.add_parser("token", help="Token operations")
        token_parser_subparsers = token_parser.add_subparsers(help="Token operations", dest='subcommand')

        # token exchange
        token_exchange_parser = token_parser_subparsers.add_parser("exchange", help="Exchange an existing token for a "
                                                                                    "different service")
        token_exchange_parser.add_argument('service', help="The service name to exchange for")

        # token get
        token_get_parser = token_parser_subparsers.add_parser("get", help="Get contents of an existing token")
        token_get_parser.add_argument('service', help="The service name")

        # token ls
        token_parser_subparsers.add_parser("ls", help="List existing tokens")

        # token inspect
        token_inspect_parser = token_parser_subparsers.add_parser("inspect", help="Inspect an existing token")
        token_inspect_parser.add_argument('service', help="The service name")

        # token request
        token_parser_subparsers.add_parser("request", help="Request a new token")

        return (token_parser,)

    parser = argparse.ArgumentParser(description="Operator command line utilities to interface with SRCNet APIs.")

    parser.add_argument('-c', help="path to configuration file", type=str, default=[
        'etc/srcnet-clients-config.yml', 
        os.path.join(Path.home(), '.local', 'etc', 'srcnet-clients-config.yml'), 
        '/usr/local/etc/srcnet-clients-config.yml'])
    parser.add_argument("--debug", help="debug mode", action='store_true')
    parser.add_argument("--json", help="output as json", action='store_true')

    subparsers = parser.add_subparsers(help='', dest='command')

    parsers = [
        parser,
        *add_api_commands_to_subparsers(subparsers),
        *add_config_commands_to_subparsers(subparsers),
        *add_data_commands_to_subparsers(subparsers),
        *add_metadata_commands_to_subparsers(subparsers),
        *add_site_commands_to_subparsers(subparsers),
        *add_token_commands_to_subparsers(subparsers)
    ]
    parsers_by_prog = {}
    for parser in parsers:
        parsers_by_prog[parser.prog] = parser

    if return_all_parsers:
        return Box(parsers_by_prog)
    return parsers[0]


if __name__ == "__main__":
    parsers = make_parsers(return_all_parsers=True)
    args = parsers.srcnet_oper.parse_args()

    # Set up a logger.
    if args.debug:
        logging.basicConfig(
            level=logging.DEBUG,
            format="%(asctime)s [%(name)s] %(module)10s %(levelname)5s %(process)d\t%(message)s")
    else:
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(name)s] %(module)10s %(levelname)5s %(process)d\t%(message)s")

    if not isinstance(args.c, list):
        args.c = [args.c]

    for file_path in args.c:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r') as file:
                    config = yaml.safe_load(file)
                break
            except yaml.YAMLError:
                continue

    if config is not None:
        if 'apis' not in config:
            logging.critical("Unable to find apis key in config file. This is required.")
            exit()
    else:
        logging.critical("Config file is empty.")
        exit()
    
    # Instantiate and set up the session.
    session = OIDCSession(config=config)
    session.load_tokens_from_disk()
    
    output = {
        'raw': None,
        'tabulated': None
    }
    if args.command == 'api':
        if args.subcommand == 'health':
            output['raw'] = ServicesAPI(session=session).health(service=args.service)
        elif args.subcommand == 'ls':
            output['raw'] = sorted(list(config.get("apis").keys()))
        elif args.subcommand == 'ping':
            output['raw'] = ServicesAPI(session=session).ping(service=args.service)
        else:
            parsers.srcnet_oper_api.print_help()
    elif args.command == 'config':
        if args.subcommand == 'get':
            if args.section:
                output['raw'] = config.get(args.section)
                if not output['raw']:
                    output['raw'] = "Section with name {} not found".format(args.section)
            else:
                output['raw'] = config
        else:
            parsers.srcnet_oper_config.print_help()
    elif args.command == 'data':
        if args.subcommand == 'download':
            output['raw'] = DataAPI(session=session).download(
                namespace=args.namespace,
                name=args.name,
                sort=args.sort,
                ip_address=args.ip_address,
                output_filename=args.output
            )
        elif args.subcommand == 'locate':
            output['raw'] = DataAPI(session=session).locate(
                namespace=args.namespace,
                name=args.name,
                sort=args.sort,
                ip_address=args.ip_address
            )

            # make a plot if requested
            if args.plot:
                # get the storage lat/longs
                storages_by_site = SiteAPI(session=session).list_storages()
                data_by_host = []
                for site in storages_by_site:
                    for storage in site.get('storages', []):
                        host = storage.get('host')
                        latitude = storage.get('latitude')
                        longitude = storage.get('longitude')
                        if host and latitude and longitude:
                            count = len([entry for entry in output['raw'] if url_to_parts(entry).get('host') == host])
                            if count > 0:
                                data_by_host.append({
                                    'identifier': host,
                                    'latitude': latitude,
                                    'longitude': longitude,
                                    'count': count,
                                    'label': "{}: {}".format(host, count)
                                })

                # plot
                fig = go.Figure()
                plot_scatter_world_map(fig=fig, data=data_by_host, latitude_key='latitude', longitude_key='longitude',
                                       value_key='count', label_key='label', size_offset=10)
                fig.show()
        elif args.subcommand == 'ls':
            output['raw'] = DataAPI(session=session).list_files_in_namespace(
                namespace=args.namespace,
                name=args.name,
                detail=args.detail,
                filters=args.filters,
                limit=args.limit
            )
        elif args.subcommand == 'rules':
            if args.subsubcommand == 'did':
                output['raw'] = DataAPI(session=session).list_rules_for_data_identifier(
                    namespace=args.namespace,
                    name=args.name
                )
                # make a nice table
                table = PrettyTable()
                table.field_names = ["Rule ID", "Scope", "Name", "RSE Expression", "Copies", "State", "Created At",
                                     "Updated At", "Expires At", ]
                table.align = 'l'

                for entry in output['raw'].get('rules'):
                    table.add_row([
                        entry.get('id'),
                        entry.get('scope'),
                        entry.get('name'),
                        entry.get('rse_expression'),
                        entry.get('copies'),
                        entry.get('state'),
                        entry.get('created_at'),
                        entry.get('updated_at'),
                        entry.get('expired_at'),
                    ])
                output['tabulated'] = table

                # make a plot if requested
                if args.plot:
                    # get the storage area lat/longs
                    storage_areas_topojson = SiteAPI(session=session).list_storage_areas_topojson()
                    geometries = storage_areas_topojson.get('objects', {}).get('sites', {}).get('geometries', [])
                    data_by_storage_area = []
                    for geometry in geometries:
                        coordinates = geometry.get('coordinates')
                        identifier = geometry.get('properties', {}).get('name')
                        if identifier and coordinates:
                            rules_for_this_identifier = [entry for entry in output['raw'].get('rules')
                                                         if entry.get('rse_expression') == identifier]
                            count_ALL = len(rules_for_this_identifier)
                            count_OK = len([rule for rule in rules_for_this_identifier if rule.get('state') == 'OK'])
                            count_REPL = len([rule for rule in rules_for_this_identifier
                                          if rule.get('state') == 'REPLICATING'])
                            count_STUCK = len([rule for rule in rules_for_this_identifier
                                               if rule.get('state') == 'STUCK'])
                            if count_ALL > 0:
                                data_by_storage_area.append({
                                    'identifier': identifier,
                                    'latitude': coordinates[1],
                                    'longitude': coordinates[0],
                                    'count': count_ALL,
                                    'label': "<b>{}</b><br>Total: {}<br>OK: {}<br>Replicating: {}<br>Stuck: {}".format(
                                        identifier, count_ALL, count_OK, count_REPL, count_STUCK)
                                })

                    # plot
                    fig = go.Figure()
                    plot_scatter_world_map(fig=fig, data=data_by_storage_area, latitude_key='latitude',
                                           longitude_key='longitude', value_key='count', label_key='label',
                                           size_offset=10)
                    fig.show()
            elif args.subsubcommand == 'ls':
                output['raw'] = DataAPI(session=session).list_rules_for_namespace(
                    namespace=args.namespace,
                    datetime_from=args.datetime_from,
                    datetime_to=args.datetime_to,
                    limit=args.limit
                )

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Rule ID", "Scope", "Name", "RSE Expression", "Copies", "State", "Created At",
                                     "Updated At", "Expires At", ]
                table.align = 'l'

                for entry in output['raw'].get('rules'):
                    table.add_row([
                        entry.get('id'),
                        entry.get('scope'),
                        entry.get('name'),
                        entry.get('rse_expression'),
                        entry.get('copies'),
                        entry.get('state'),
                        entry.get('created_at'),
                        entry.get('updated_at'),
                        entry.get('expired_at')
                    ])
                output['tabulated'] = table

                # make a plot if requested
                if args.plot:
                    # get the storage area lat/longs
                    storage_areas_topojson = SiteAPI(session=session).list_storage_areas_topojson()
                    geometries = storage_areas_topojson.get('objects', {}).get('sites', {}).get('geometries', [])
                    data_by_storage_area = []
                    for geometry in geometries:
                        coordinates = geometry.get('coordinates')
                        identifier = geometry.get('properties', {}).get('name')
                        if identifier and coordinates:
                            rules_for_this_identifier = [entry for entry in output['raw'].get('rules')
                                                         if entry.get('rse_expression') == identifier]
                            count_ALL = len(rules_for_this_identifier)
                            count_OK = len([rule for rule in rules_for_this_identifier if rule.get('state') == 'OK'])
                            count_REPL = len([rule for rule in rules_for_this_identifier
                                          if rule.get('state') == 'REPLICATING'])
                            count_STUCK = len([rule for rule in rules_for_this_identifier
                                               if rule.get('state') == 'STUCK'])
                            if count_ALL > 0:
                                data_by_storage_area.append({
                                    'identifier': identifier,
                                    'latitude': coordinates[1],
                                    'longitude': coordinates[0],
                                    'count': count_ALL,
                                    'label': "<b>{}</b><br>Total: {}<br>OK: {}<br>Replicating: {}<br>Stuck: {}".format(
                                        identifier, count_ALL, count_OK, count_REPL, count_STUCK)
                                })

                    # plot
                    fig = go.Figure()
                    plot_scatter_world_map(fig=fig, data=data_by_storage_area, latitude_key='latitude',
                                           longitude_key='longitude', value_key='count', label_key='label',
                                           size_offset=10)
                    fig.show()
            else:
                parsers.srcnet_oper_data_rules.print_help()
        elif args.subcommand == 'upload':
            if args.subsubcommand == 'ingest':
                DataAPI(session=session).upload_for_ingest(
                    path=args.path,
                    ingest_service_id=args.ingest_service_id,
                    namespace=args.namespace,
                    metadata_suffix=args.metadata_suffix,
                    extra_metadata=args.extra_metadata,
                    debug=args.debug
                )
            else:
                parsers.srcnet_oper_data_upload.print_help()
        elif args.subcommand == 'namespace':
            if args.subsubcommand == 'ls':
                output['raw'] = DataAPI(session=session).list_namespaces()
            else:
                parsers.srcnet_oper_data_namespace.print_help()
        else:
            parsers.srcnet_oper_data.print_help()
    elif args.command == 'metadata':
        if args.subcommand == 'get':
            plugins = []
            if ',' in args.store:
                stores = args.store.split(',')
            else:
                stores = args.store
            if 'file' in stores:
                plugins.append("DID_COLUMN")
            if 'science' in stores:
                plugins.append("POSTGRES_JSON")

            outputs = {}
            for plugin in plugins:
                outputs[plugin] = MetadataAPI(session=session).get_metadata(
                    namespace=args.namespace,
                    name=args.name,
                    plugin=plugin
                )
            if outputs:
                output['raw'] = outputs

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Store", "Key", "Value"]
                table.align = 'l'

                for plugin, metadata in outputs.items():
                    for key, value in metadata.items():
                        if value is None and not args.showempty:
                            continue
                        table.add_row([
                            plugin,
                            key,
                            value
                        ])
                output['tabulated'] = table
        elif args.subcommand == 'set':
            output['raw'] = MetadataAPI(session=session).set_metadata(
                    namespace=args.namespace,
                    name=args.name,
                    metadata=args.metadata
                )
        else:
            parsers.srcnet_oper_metadata.print_help()
    elif args.command == 'site':
        if args.subcommand == 'add':
            url = SiteAPI(session=session).get_add_site_www_url()
            print("Please navigate to: {url}".format(url=url))
        elif args.subcommand == 'edit':
            url = SiteAPI(session=session).get_edit_site_www_url(args.site)
            print("Please navigate to: {url}".format(url=url))
        elif args.subcommand == 'ls':
            output['raw'] = SiteAPI(session=session).list_sites()
        elif args.subcommand == 'compute':
            if args.subsubcommand == 'get':
                output['raw'] = SiteAPI(session=session).get_compute(compute_id=args.id)

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Identifier", "ID", "Latitude", "Longitude", "Description"]
                table.align = 'l'

                table.add_row([
                    output['raw'].get('identifier'),
                    output['raw'].get('id'),
                    output['raw'].get('latitude'),
                    output['raw'].get('longitude'),
                    output['raw'].get('description')
                ])
                output['tabulated'] = table
            elif args.subsubcommand == 'ls':
                output['raw'] = SiteAPI(session=session).list_compute()

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Site", "Identifier", "ID", "Latitude", "Longitude", "Description"]
                table.align = 'l'

                for entry in output['raw']:
                    for compute_attributes in entry.get('compute'):
                        table.add_row([
                            entry.get('site_name'),
                            compute_attributes.get('identifier'),
                            compute_attributes.get('id'),
                            compute_attributes.get('latitude'),
                            compute_attributes.get('longitude'),
                            compute_attributes.get('description'),
                        ])
                output['tabulated'] = table
            else:
                parsers.srcnet_oper_site_compute.print_help()
        elif args.subcommand == 'service':
            if args.subsubcommand == 'get':
                output['raw'] = SiteAPI(session=session).get_service(service_id=args.id)

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Identifier", "Type", "Version", "Enabled?", "Mandatory?", "ID", "Prefix", "Host",
                                     "Port", "Path"]
                table.align = 'l'

                table.add_row([
                    output['raw'].get('identifier'),
                    output['raw'].get('type'),
                    output['raw'].get('version'),
                    u'\u2713' if output['raw'].get('enabled') else '',
                    u'\u2713' if output.get('is_mandatory') else '',
                    output['raw'].get('id'),
                    output['raw'].get('prefix'),
                    output['raw'].get('host'),
                    output['raw'].get('port'),
                    output['raw'].get('path')
                ])
                output['tabulated'] = table
            elif args.subsubcommand == 'ls':
                output['raw'] = SiteAPI(session=session).list_services(service_type=args.type)

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Site", "Identifier", "Type", "Version", "Enabled?", "Mandatory?", "ID", "Prefix",
                                     "Host", "Port", "Path"]
                table.align = 'l'

                for entry in output['raw']:
                    for service_attributes in entry.get('services'):
                        table.add_row([
                            entry.get('site_name'),
                            service_attributes.get('identifier'),
                            service_attributes.get('type'),
                            service_attributes.get('version'),
                            u'\u2713' if service_attributes.get('enabled') else '',
                            u'\u2713' if service_attributes.get('is_mandatory') else '',
                            service_attributes.get('id'),
                            service_attributes.get('prefix'),
                            service_attributes.get('host'),
                            service_attributes.get('port'),
                            service_attributes.get('path')
                        ])
                output['tabulated'] = table
            elif args.subsubcommand == 'types':
                output['raw'] = SiteAPI(session=session).list_service_types()

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Global or local?", "Name"]
                table.align = 'l'

                for entry in output['raw'].get('global'):
                    table.add_row([
                        "Global",
                        entry
                    ])
                for entry in output['raw'].get('local'):
                    table.add_row([
                        "Local",
                        entry
                    ])
                output['tabulated'] = table
            else:
                parsers.srcnet_oper_site_service.print_help()
        elif args.subcommand == 'storage':
            if args.subsubcommand == 'get':
                output['raw'] = SiteAPI(session=session).get_storage(storage_id=args.id)

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Identifier", "ID", "Latitude", "Longitude", "Size (TB)", "SRM"]
                table.align = 'l'

                table.add_row([
                    output['raw'].get('identifier'),
                    output['raw'].get('id'),
                    output['raw'].get('latitude'),
                    output['raw'].get('longitude'),
                    output['raw'].get('size_in_terabytes'),
                    output['raw'].get('srm')
                ])
                output['tabulated'] = table
            elif args.subsubcommand == 'ls':
                output['raw'] = SiteAPI(session=session).list_storages()

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Site", "Identifier", "ID", "Latitude", "Longitude", "Size (TB)", "SRM"]
                table.align = 'l'

                for entry in output['raw']:
                    for storage_attributes in entry.get('storages'):
                        table.add_row([
                            entry.get('site_name'),
                            storage_attributes.get('identifier'),
                            storage_attributes.get('id'),
                            storage_attributes.get('latitude'),
                            storage_attributes.get('longitude'),
                            storage_attributes.get('size_in_terabytes'),
                            storage_attributes.get('srm')
                        ])
                output['tabulated'] = table
            else:
                parsers.srcnet_oper_site_storage.print_help()
        elif args.subcommand == 'storage-area':
            if args.subsubcommand == 'get':
                output['raw'] = SiteAPI(session=session).get_storage_area(storage_area_id=args.id)

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Identifier", "ID", "Type"]
                table.align = 'l'

                table.add_row([
                    output['raw'].get('identifier'),
                    output['raw'].get('id'),
                    output['raw'].get('type')
                ])
                output['tabulated'] = table
            elif args.subsubcommand == 'ls':
                output['raw'] = SiteAPI(session=session).list_storage_areas()

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Site", "Identifier", "ID", "Type"]
                table.align = 'l'

                for entry in output['raw']:
                    for storage_attributes in entry.get('storage_areas'):
                        table.add_row([
                            entry.get('site_name'),
                            storage_attributes.get('identifier'),
                            storage_attributes.get('id'),
                            storage_attributes.get('type')
                        ])
                output['tabulated'] = table
            else:
                parsers.srcnet_oper_site_storage_area.print_help()
        else:
            parsers.srcnet_oper_site.print_help()
    elif args.command == 'token':
        if args.subcommand == 'exchange':
            session.exchange_token(service_name=args.service, version="latest")
        elif args.subcommand == 'get':
            access_token = session.get_access_token(service_name=args.service)
            if access_token:
                output['raw'] = access_token
            else:
                output['raw'] = "Token for service {} not found.".format(args.service)
        elif args.subcommand == 'ls':
            output['raw'] = session.list_access_tokens()

            # make a nice table
            table = PrettyTable()
            table.field_names = ["Service Name", "Access token", "Expires at (UTC)", "Expires at (Local)",
                                 "Path on disk", "Has associated refresh token?"]
            table.align = 'l'
            table.align["Has associated refresh token?"] = 'c'

            for aud, attributes in output['raw'].items():
                table.add_row([
                    aud,
                    attributes.get('access_token')[0:20],
                    datetime.datetime.fromtimestamp(attributes.get('expires_at'), datetime.timezone.utc),
                    datetime.datetime.fromtimestamp(attributes.get('expires_at')),
                    attributes.get('path_on_disk'),
                    u'\u2713' if attributes.get('has_associated_refresh_token') else ''
                ])
            output['tabulated'] = table
        elif args.subcommand == 'inspect':
            output['raw'] = session.inspect_access_token(service_name=args.service)
        elif args.subcommand == 'request':
            session.start_device_flow()
        else:
            parsers.srcnet_oper_token.print_help()
    else:
        parsers.srcnet_oper.print_help()

    if args.json:
        if output.get('raw'):
            print(json.dumps(output.get('raw')))
    else:
        if output.get('tabulated'):
            print(output.get('tabulated'))
        elif output.get('raw'):
            pprint.pprint(output.get('raw'))
