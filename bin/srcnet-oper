#!/usr/bin/env python

import argparse
import datetime
import json
import logging
import os
import yaml
from pathlib import Path

import plotly.graph_objects as go
import pprint
from box import Box
from prettytable import PrettyTable

from ska_src_clients.api import DataAPI, MetadataAPI, ServicesAPI, SiteAPI
from ska_src_clients.common.utility import plot_scatter_world_map, url_to_parts
from ska_src_clients.session.oidc import OIDCSession


def make_parsers(return_all_parsers=False):
    def add_api_commands_to_subparsers(subparsers):
        api_parser = subparsers.add_parser('api', help="API operations")
        api_parser_subparsers = api_parser.add_subparsers(help="API operations", dest='subcommand')

        # api health
        api_health_parser = api_parser_subparsers.add_parser("health", help="Get the API health")
        api_health_parser.add_argument('service', help="The service name")

        # api ls
        api_parser_subparsers.add_parser("ls", help="List APIs")

        # api ping
        api_health_parser = api_parser_subparsers.add_parser("ping", help="Ping the API")
        api_health_parser.add_argument('service', help="The service name")

        return (api_parser,)

    def add_config_commands_to_subparsers(subparsers):
        config_parser = subparsers.add_parser("config", help="Configuration file operations")
        config_parser_subparsers = config_parser.add_subparsers(help="Configuration file operations", dest='subcommand')

        # config get
        config_get_parser = config_parser_subparsers.add_parser("get", help="Get a configuration file")
        config_get_parser.add_argument('--section', help="Configuration file section to return", required=True)

        return (config_parser,)

    def add_data_commands_to_subparsers(subparsers):
        data_parser = subparsers.add_parser("data", help="Data operations")
        data_parser_subparsers = data_parser.add_subparsers(help="Data operations", dest='subcommand')

        # data download
        data_download_parser = data_parser_subparsers.add_parser(
            "download", help="Download a replica of a data identifier.")
        data_download_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_download_parser.add_argument('--name', default='*', help="The data identifier name (wildcards allowed)",
                                    required=True)
        data_download_parser.add_argument('--sort', default="nearest_by_ip", help="The sorting algorithm to use "
                                                                                   "(random || nearest_by_ip)")
        data_download_parser.add_argument('--ip_address', default="", help="The ip address to geolocate the nearest "
                                                                           "replica to. Leave blank to use the "
                                                                           "requesting client ip "
                                                                           "(sort == nearest_by_ip only)")
        data_download_parser.add_argument('--no-verify', action="store_true", help="Disable verification of the "
                                                                                    "server's SSL certificate?")
        data_download_parser.add_argument('--output', help="The output filename")

        # data locate
        data_locate_parser = data_parser_subparsers.add_parser("locate", help="Locate replicas of data identifiers")
        data_locate_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_locate_parser.add_argument('--name', default='*', help="The data identifier name (wildcards allowed)",
                                          required=True)
        data_locate_parser.add_argument('--sort', default="nearest_by_ip", help="The sorting algorithm to use "
                                                                                "(random || nearest_by_ip)")
        data_locate_parser.add_argument('--ip-address', default="", help="The ip address to geolocate the nearest "
                                                                         "requesting client ip "
                                                                         "(sort == nearest_by_ip only)")
        data_locate_parser.add_argument('--plot', action="store_true", help="Plot?")

        # data ls
        data_ls_parser = data_parser_subparsers.add_parser("ls", help="List data identifiers in a namespace")
        data_ls_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_ls_parser.add_argument('--name', default='*', help="The data identifier name (wildcards allowed)",
                                    required=True)
        data_ls_parser.add_argument('--detail', action="store_true", help="More detail?")
        data_ls_parser.add_argument('--filters', default=None, help="Filters (Rucio only)")
        data_ls_parser.add_argument('--limit', default=100, help="Number of identifiers to return in result")

        # data move
        data_move_parser = data_parser_subparsers.add_parser("move",
                                                             help="Data movement operations")
        data_move_parser_subparsers = data_move_parser.add_subparsers(
            help="Data movement operations", dest='subsubcommand')

        # data move request
        data_move_request_parser = data_move_parser_subparsers.add_parser("request",
                                                                          help="Request movement of existing data to another storage area")
        data_move_request_parser.add_argument('--to-storage-area-uuid',
                                              help="The storage area uuid to move data to",
                                              required=True)
        data_move_request_parser.add_argument('--dids', nargs="+", help="The list of DIDs to move",
                                              required=True)
        data_move_request_parser.add_argument('--lifetime',
                                              help="The lifetime of the data in seconds",
                                              required=True)
        data_move_request_parser.add_argument('--parent-namespace',
                                              help="The parent container namespace. Defaults to using the first DID's namespace (Rucio only)",
                                              required=False)

        # data move status
        data_move_status_parser = data_move_parser_subparsers.add_parser("status",
                                                                         help="Get the status of a data movement request")
        data_move_status_parser.add_argument('--job-id', help="The job id", required=True)

        # data namespace
        data_namespace_parser = data_parser_subparsers.add_parser("namespace", help="Data namespace operations")
        data_namespace_parser_subparsers = data_namespace_parser.add_subparsers(help="Data namespace operations",
                                                                                dest='subsubcommand')

        # data namespace ls
        data_namespace_parser_subparsers.add_parser("ls", help="List namespaces")

        # data stage
        data_stage_parser = data_parser_subparsers.add_parser("stage",
                                                              help="Data staging operations")
        data_stage_parser_subparsers = data_stage_parser.add_subparsers(
            help="Data staging operations", dest='subsubcommand')

        # data stage request
        data_stage_request_parser = data_stage_parser_subparsers.add_parser("request",
                                                                            help="Request staging of existing data in a storage area")
        data_stage_request_parser.add_argument('--to-storage-area-uuid',
                                               help="The storage area uuid to stage data in",
                                               required=True)
        data_stage_request_parser.add_argument('--dids', nargs="+", help="The list of DIDs to move",
                                               required=True)
        data_stage_request_parser.add_argument('--lifetime',
                                               help="The lifetime of the data in seconds",
                                               required=True)
        data_stage_request_parser.add_argument('--parent-namespace',
                                               help="The parent container namespace. Defaults to using the first DID's namespace (Rucio only)",
                                               required=False)

        # data stage status
        data_stage_status_parser = data_stage_parser_subparsers.add_parser("status",
                                                                           help="Get the status of a data staging request")
        data_stage_status_parser.add_argument('--job-id', help="The job id", required=True)

        # data upload
        data_upload_parser = data_parser_subparsers.add_parser("upload", help="Data upload operations")
        data_upload_parser_subparsers = data_upload_parser.add_subparsers(help="Data upload operations",
                                                                          dest='subsubcommand')

        # data upload ingest
        data_upload_ingest_parser = data_upload_parser_subparsers.add_parser("ingest",
                                                                             help="Upload data to ingest area")
        data_upload_ingest_parser.add_argument('--path', help="Local path to data directory to be uploaded",
                                               required=True)
        data_upload_ingest_parser.add_argument('--ingest-service-id', help="The ingest service id", required=True)
        data_upload_ingest_parser.add_argument('--namespace', help="The data namespace", required=True)
        data_upload_ingest_parser.add_argument('--extra-metadata', help="Extra metadata to apply to each file (JSON)",
                                               default='{}')
        data_upload_ingest_parser.add_argument('--metadata-suffix', help="The expected metadata suffix",
                                               default='.meta')

        return data_parser, data_move_parser, data_namespace_parser, data_stage_parser, data_upload_parser

    def add_metadata_commands_to_subparsers(subparsers):
        metadata_parser = subparsers.add_parser("metadata", help="Metadata operations")
        metadata_parser_subparsers = metadata_parser.add_subparsers(help="Metadata operations", dest='subcommand')

        # metadata get
        metadata_get_parser = metadata_parser_subparsers.add_parser("get", help="Get metadata for a data identifier")
        metadata_get_parser.add_argument('--namespace', help="The data identifier's namespace", required=True)
        metadata_get_parser.add_argument('--name', help="The data identifier's name", required=True)
        metadata_get_parser.add_argument('--store', default="file", help="The metadata store to use (file || science)")
        metadata_get_parser.add_argument('--showempty', action="store_true", help="Show empty keys?")

        # metadata set
        metadata_set_parser = metadata_parser_subparsers.add_parser("set", help="Set metadata for a data identifier")
        metadata_set_parser.add_argument('--namespace', help="The data identifier's namespace", required=True)
        metadata_set_parser.add_argument('--name', help="The data identifier's name", required=True)
        metadata_set_parser.add_argument('--metadata', help="Metadata to set (JSON)", required=True)

        return (metadata_parser,)

    def add_site_commands_to_subparsers(subparsers):
        site_parser = subparsers.add_parser("site", help="Site operations")
        site_parser_subparsers = site_parser.add_subparsers(help="Site operations", dest='subcommand')

        # site add
        site_parser_subparsers.add_parser("add", help="Add a new site")

        # site edit
        site_edit_parser = site_parser_subparsers.add_parser("edit", help="Edit an existing site")
        site_edit_parser.add_argument('--node', help="Name of the node to edit", required=True)

        # site get
        site_get_parser = site_parser_subparsers.add_parser("get", help="Get site by ID")
        site_get_parser.add_argument('--id', help="ID of the site to get", required=True)

        # site ls
        site_parser_subparsers.add_parser("ls", help="List sites")

        # site compute
        site_compute_parser = site_parser_subparsers.add_parser("compute", help="Site compute operations")
        site_compute_parser_subparsers = site_compute_parser.add_subparsers(help="Site compute operations",
                                                                            dest='subsubcommand')

        # site compute get
        site_compute_get_parser = site_compute_parser_subparsers.add_parser(
            "get", help="Get description of a compute element")
        site_compute_get_parser.add_argument('--id', help="ID of the compute element to get", required=True)
        site_compute_get_parser.add_argument('--services', help="Show associated services", action="store_true")

        # site compute ls
        site_compute_ls_parser = site_compute_parser_subparsers.add_parser("ls", help="List compute elements")
        site_compute_ls_parser.add_argument('--node', help="Filter by node name")
        site_compute_ls_parser.add_argument('--site', help="Filter by site name")

        # site service
        site_service_parser = site_parser_subparsers.add_parser("service", help="Site service operations")
        site_service_parser_subparsers = site_service_parser.add_subparsers(help="Site service operations",
                                                                            dest='subsubcommand')


        # site service enable
        site_service_enable_parser = site_service_parser_subparsers.add_parser(
            "enable", help="Enable a service")
        site_service_enable_parser.add_argument('--service-uuid', help="UUID of the service to enable", required=True)

        # site service disable
        site_service_enable_parser = site_service_parser_subparsers.add_parser(
            "disable", help="Disable a service")
        site_service_enable_parser.add_argument('--service-uuid', help="UUID of the service to disable", required=True)

        # site service get
        site_service_get_parser = site_service_parser_subparsers.add_parser(
            "get", help="Get description of a service")
        site_service_get_parser.add_argument('--id', help="ID of the service to get", required=True)

        # site service ls
        site_service_ls_parser = site_service_parser_subparsers.add_parser("ls", help="List services")
        site_service_ls_parser.add_argument('--type', help="Filter for type of service (accepts wildcards)")
        site_service_ls_parser.add_argument('--node', help="Filter by node name")
        site_service_ls_parser.add_argument('--site', help="Filter by site name")
        site_service_ls_parser.add_argument(
            '--scope',
            help="Filter by service scope (all, local, global)",
            choices=['all', 'local', 'global'],
            default='all'
        )

        # site service types
        site_service_parser_subparsers.add_parser("types", help="List service types")

        # site storage
        site_storage_parser = site_parser_subparsers.add_parser("storage", help="Site storage operations")
        site_storage_parser_subparsers = site_storage_parser.add_subparsers(help="Site storage operations",
                                                                            dest='subsubcommand')

        # site storage get
        site_storage_get_parser = site_storage_parser_subparsers.add_parser(
            "get", help="Get description of a storage resource")
        site_storage_get_parser.add_argument('--id', help="ID of the storage resource to get", required=True)
        site_storage_get_parser.add_argument('--areas', help="Show storage areas", action="store_true")

        # site storage ls
        site_storage_ls_parser = site_storage_parser_subparsers.add_parser(
            "ls", help="List storages")
        site_storage_ls_parser.add_argument('--node', help="Filter by node name")
        site_storage_ls_parser.add_argument('--site', help="Filter by site name")

        # site storage area
        site_storage_area_parser = site_parser_subparsers.add_parser("storage-area",
                                                                     help="Site storage area operations")
        site_storage_area_parser_subparsers = site_storage_area_parser.add_subparsers(
            help="Site storage area operations", dest='subsubcommand')

        # site storage area get
        site_storage_area_get_parser = site_storage_area_parser_subparsers.add_parser(
            "get", help="Get description of a storage area resource")
        site_storage_area_get_parser.add_argument('--id', help="ID of the storage area_resource to get", required=True)

        # site storage area ls
        site_storage_area_ls_parser = site_storage_area_parser_subparsers.add_parser(
            "ls", help="List storage areas")
        site_storage_area_ls_parser.add_argument('--node', help="Filter by node name")
        site_storage_area_ls_parser.add_argument('--site', help="Filter by site name")

        return site_parser, site_compute_parser, site_service_parser, site_storage_parser, site_storage_area_parser

    def add_token_commands_to_subparsers(subparsers):
        token_parser = subparsers.add_parser("token", help="Token operations")
        token_parser_subparsers = token_parser.add_subparsers(help="Token operations", dest='subcommand')

        # token exchange
        token_exchange_parser = token_parser_subparsers.add_parser("exchange", help="Exchange an existing token for a "
                                                                                    "different service")
        token_exchange_parser.add_argument('service', help="The service name to exchange for")

        # token get
        token_get_parser = token_parser_subparsers.add_parser("get", help="Get contents of an existing token")
        token_get_parser.add_argument('service', help="The service name")

        # token ls
        token_parser_subparsers.add_parser("ls", help="List existing tokens")

        # token inspect
        token_inspect_parser = token_parser_subparsers.add_parser("inspect", help="Inspect an existing token")
        token_inspect_parser.add_argument('service', help="The service name")

        # token request
        token_parser_subparsers.add_parser("request", help="Request a new token")

        return (token_parser,)

    parser = argparse.ArgumentParser(description="Operator command line utilities to interface with SRCNet APIs.")

    parser.add_argument('-c', help="path to configuration file", type=str, default=[
        'etc/srcnet-clients-config.yml', 
        os.path.join(Path.home(), '.local', 'etc', 'srcnet-clients-config.yml'), 
        '/usr/local/etc/srcnet-clients-config.yml'])
    parser.add_argument("--debug", help="debug mode", action='store_true')
    parser.add_argument("--json", help="output as json", action='store_true')

    subparsers = parser.add_subparsers(help='', dest='command')

    parsers = [
        parser,
        *add_api_commands_to_subparsers(subparsers),
        *add_config_commands_to_subparsers(subparsers),
        *add_data_commands_to_subparsers(subparsers),
        *add_metadata_commands_to_subparsers(subparsers),
        *add_site_commands_to_subparsers(subparsers),
        *add_token_commands_to_subparsers(subparsers)
    ]
    parsers_by_prog = {}
    for parser in parsers:
        parsers_by_prog[parser.prog] = parser

    if return_all_parsers:
        return Box(parsers_by_prog)
    return parsers[0]


if __name__ == "__main__":
    parsers = make_parsers(return_all_parsers=True)
    args = parsers.srcnet_oper.parse_args()

    # Set up a logger.
    if args.debug:
        logging.basicConfig(
            level=logging.DEBUG,
            format="%(asctime)s [%(name)s] %(module)10s %(levelname)5s %(process)d\t%(message)s")
    else:
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s [%(name)s] %(module)10s %(levelname)5s %(process)d\t%(message)s")

    if not isinstance(args.c, list):
        args.c = [args.c]

    for file_path in args.c:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r') as file:
                    config = yaml.safe_load(file)
                break
            except yaml.YAMLError:
                continue

    if config is not None:
        if 'apis' not in config:
            logging.critical("Unable to find apis key in config file. This is required.")
            exit()
    else:
        logging.critical("Config file is empty.")
        exit()
    
    # Instantiate and set up the session.
    session = OIDCSession(config=config)
    session.load_tokens_from_disk()
    
    output = {
        'raw': None,
        'tabulated': None
    }
    if args.command == 'api':
        if args.subcommand == 'health':
            output['raw'] = ServicesAPI(session=session).health(service=args.service)
        elif args.subcommand == 'ls':
            output['raw'] = sorted(list(config.get("apis").keys()))
        elif args.subcommand == 'ping':
            output['raw'] = ServicesAPI(session=session).ping(service=args.service)
        else:
            parsers.srcnet_oper_api.print_help()
    elif args.command == 'config':
        if args.subcommand == 'get':
            if args.section:
                output['raw'] = config.get(args.section)
                if not output['raw']:
                    output['raw'] = "Section with name {} not found".format(args.section)
            else:
                output['raw'] = config
        else:
            parsers.srcnet_oper_config.print_help()
    elif args.command == 'data':
        if args.subcommand == 'move':
            if args.subsubcommand == 'request':
                output['raw'] = DataAPI(session=session).move_request(
                    to_storage_area_uuid=args.to_storage_area_uuid,
                    dids=args.dids,
                    lifetime=args.lifetime,
                    parent_namespace=args.parent_namespace
                )
            elif args.subsubcommand == 'status':
                output['raw'] = DataAPI(session=session).move_status(
                    job_id=args.job_id
                )
            else:
                parsers.srcnet_oper_data_move.print_help()
        elif args.subcommand == 'stage':
            if args.subsubcommand == 'request':
                output['raw'] = DataAPI(session=session).stage_request(
                    to_storage_area_uuid=args.to_storage_area_uuid,
                    dids=args.dids,
                    lifetime=args.lifetime,
                    parent_namespace=args.parent_namespace
                )
            elif args.subsubcommand == 'status':
                output['raw'] = DataAPI(session=session).stage_status(
                    job_id=args.job_id
                )
            else:
                parsers.srcnet_oper_data_stage.print_help()
        elif args.subcommand == 'download':
            output['raw'] = DataAPI(session=session).download(
                namespace=args.namespace,
                name=args.name,
                sort=args.sort,
                ip_address=args.ip_address,
                verify=not args.no_verify,
                output_filename=args.output
            )
        elif args.subcommand == 'locate':
            output['raw'] = DataAPI(session=session).locate(
                namespace=args.namespace,
                name=args.name,
                sort=args.sort,
                ip_address=args.ip_address
            )

            # make a plot if requested
            if args.plot:
                # get the storage lat/longs
                storages_by_site = SiteAPI(session=session).list_storages()
                data_by_host = []
                for site in storages_by_site:
                    for storage in site.get('storages', []):
                        host = storage.get('host')
                        latitude = storage.get('latitude')
                        longitude = storage.get('longitude')
                        if host and latitude and longitude:
                            count = len([entry for entry in output['raw'] if url_to_parts(entry).get('host') == host])
                            if count > 0:
                                data_by_host.append({
                                    'identifier': host,
                                    'latitude': latitude,
                                    'longitude': longitude,
                                    'count': count,
                                    'label': "{}: {}".format(host, count)
                                })

                # plot
                fig = go.Figure()
                plot_scatter_world_map(fig=fig, data=data_by_host, latitude_key='latitude', longitude_key='longitude',
                                       value_key='count', label_key='label', size_offset=10)
                fig.show()
        elif args.subcommand == 'ls':
            output['raw'] = DataAPI(session=session).list_files_in_namespace(
                namespace=args.namespace,
                name=args.name,
                detail=args.detail,
                filters=args.filters,
                limit=args.limit
            )
        elif args.subcommand == 'upload':
            if args.subsubcommand == 'ingest':
                DataAPI(session=session).upload_for_ingest(
                    path=args.path,
                    ingest_service_id=args.ingest_service_id,
                    namespace=args.namespace,
                    metadata_suffix=args.metadata_suffix,
                    extra_metadata=args.extra_metadata,
                    debug=args.debug
                )
            else:
                parsers.srcnet_oper_data_upload.print_help()
        elif args.subcommand == 'namespace':
            if args.subsubcommand == 'ls':
                output['raw'] = DataAPI(session=session).list_namespaces()
            else:
                parsers.srcnet_oper_data_namespace.print_help()
        else:
            parsers.srcnet_oper_data.print_help()
    elif args.command == 'metadata':
        if args.subcommand == 'get':
            plugins = []
            if ',' in args.store:
                stores = args.store.split(',')
            else:
                stores = args.store
            if 'file' in stores:
                plugins.append("DID_COLUMN")
            if 'science' in stores:
                plugins.append("POSTGRES_JSON")

            outputs = {}
            for plugin in plugins:
                outputs[plugin] = MetadataAPI(session=session).get_metadata(
                    namespace=args.namespace,
                    name=args.name,
                    plugin=plugin
                )
            if outputs:
                output['raw'] = outputs

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Store", "Key", "Value"]
                table.align = 'l'

                for plugin, metadata in outputs.items():
                    for key, value in metadata.items():
                        if value is None and not args.showempty:
                            continue
                        table.add_row([
                            plugin,
                            key,
                            value
                        ])
                output['tabulated'] = table
        elif args.subcommand == 'set':
            output['raw'] = MetadataAPI(session=session).set_metadata(
                    namespace=args.namespace,
                    name=args.name,
                    metadata=args.metadata
                )
        else:
            parsers.srcnet_oper_metadata.print_help()
    elif args.command == 'site':
        if args.subcommand == 'add':
            url = SiteAPI(session=session).get_add_node_www_url()
            print("Please navigate to: {url}".format(url=url))
        elif args.subcommand == 'edit':
            url = SiteAPI(session=session).get_edit_node_www_url(args.node)
            print("Please navigate to: {url}".format(url=url))
        elif args.subcommand == 'get':
            output['raw'] = SiteAPI(session=session).get_site(site_id=args.id)

            # make a nice table
            table = PrettyTable()
            table.field_names = [
                "Node", "Site", "ID", "Description", "Latitude", "Longitude",
                "Country", "Primary / Secondary Contact Email", "Comments"
            ]
            table.align = 'l'

            table.add_row([
                output["raw"].get('parent_node_name', '-'),
                output["raw"].get('name', '-'),
                output["raw"].get('id', '-'),
                output["raw"].get('description', '-'),
                output["raw"].get('latitude', '-'),
                output["raw"].get('longitude', '-'),
                output["raw"].get('country', '-'),
                "{} / {}".format(
                    output["raw"].get('primary_contact_email', '-'),
                    output["raw"].get('secondary_contact_email', '-')
                ),
                output["raw"].get('comments', '-')
            ])
            output['tabulated'] = table
        elif args.subcommand == 'ls':
            output['raw'] = SiteAPI(session=session).list_sites()

            # make a nice table
            table = PrettyTable()
            table.field_names = ["Node", "Site", "ID", "Description", "Latitude", "Longitude"]
            table.align = 'l'

            for site in output['raw']:
                table.add_row([
                    site.get('parent_node_name', '-'),
                    site.get('name', '-'),
                    site.get('id', '-'),
                    site.get('description', '-'),
                    site.get('latitude', '-'),
                    site.get('longitude', '-')
                ])
            output['tabulated'] = table

        elif args.subcommand == 'compute':
            if args.subsubcommand == 'get':
                output['raw'] = SiteAPI(session=session).get_compute(compute_id=args.id)

                # make a nice table
                field_names = [
                    "Node", "Site", "ID", "Description", "Hardware Type", "Hardware Capabilities"
                ]

                row = [
                    output["raw"].get('parent_node_name', '-'),
                    output["raw"].get('parent_site_name', '-'),
                    output["raw"].get('id', '-'),
                    output["raw"].get('description', '-'),
                    output["raw"].get('hardware_type', '-'),
                    output["raw"].get('hardware_capabilities', '-'),
                ]

                # Optionally show service info
                if args.services:
                    field_names.append("Services (ID / Type / Name)")
                    svcs = output["raw"].get('associated_local_services', [])
                    svcs_info = "\n".join("{} / {} / {}".format(
                        svc.get('id', '-'), svc.get('type', '-'), svc.get('name', '-')
                    ) for svc in svcs)
                    row.append(svcs_info)

                table = PrettyTable()
                table.field_names = field_names
                table.align = 'l'
                table.add_row(row)

                output['tabulated'] = table
                
            elif args.subsubcommand == 'ls':
                output['raw'] = SiteAPI(session=session).list_compute(
                    node_name=args.node,
                    site_name=args.site,
                )

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Node", "Site", "ID", "Description"]
                table.align = 'l'

                for entry in output['raw']:
                    table.add_row([
                        entry.get('parent_node_name', '-'),
                        entry.get('parent_site_name', '-'),
                        entry.get('id', '-'),
                        entry.get('description', '-'),
                    ])
                output['tabulated'] = table
            else:
                parsers.srcnet_oper_site_compute.print_help()
        elif args.subcommand == 'service':
            if args.subsubcommand == 'get':
                output['raw'] = SiteAPI(session=session).get_service(service_id=args.id)

                # make a nice table
                table = PrettyTable()
                table.field_names = [
                    "Node", "Site", "Name", "ID", "Type", "Scope",
                    "Parent Compute ID", "Assoc Storage ID",
                    "Prefix", "Host", "Port", "Path"
                ]
                table.align = 'l'

                table.add_row([
                    output['raw'].get('parent_node_name', '-'),
                    output['raw'].get('parent_site_name', '-'),
                    output['raw'].get('name', '-'),
                    output['raw'].get('id', '-'),
                    output['raw'].get('type', '-'),
                    output['raw'].get('scope', '-'),
                    output['raw'].get('parent_compute_id', '-'),
                    output['raw'].get('associated_storage_area_id', '-'),
                    output['raw'].get('prefix', '-'),
                    output['raw'].get('host', '-'),
                    output['raw'].get('port', '-'),
                    output['raw'].get('path', '-'),
                ])
                output['tabulated'] = table
            elif args.subsubcommand == 'enable':
                output['raw'] = SiteAPI(session=session).enable_service(service_uuid=args.service_uuid)
            elif args.subsubcommand == 'disable':
                output['raw'] = SiteAPI(session=session).disable_service(service_uuid=args.service_uuid)
            elif args.subsubcommand == 'ls':
                output['raw'] = SiteAPI(session=session).list_services(
                    service_type=args.type,
                    node_name=args.node,
                    site_name=args.site,
                    scope=args.scope
                )

                # make a nice table
                table = PrettyTable()
                table.field_names = [
                    "Node", "Site", "Name", "ID", "Type", "Scope", 
                ]
                table.align = 'l'

                for service in output['raw']:
                    table.add_row([
                        service.get('parent_node_name', '-'),
                        service.get('parent_site_name', '-'),
                        service.get('name', '-'),
                        service.get('id', '-'),
                        service.get('type', '-'),
                        service.get('scope', '-'),
                    ])
                output['tabulated'] = table
            elif args.subsubcommand == 'types':
                output['raw'] = SiteAPI(session=session).list_service_types()

                # make a nice table
                table = PrettyTable()
                table.field_names = ["Scope", "Name"]
                table.align = 'l'

                for entry in output['raw'].get('global'):
                    table.add_row([
                        "Global",
                        entry
                    ])
                for entry in output['raw'].get('local'):
                    table.add_row([
                        "Local",
                        entry
                    ])
                output['tabulated'] = table
            else:
                parsers.srcnet_oper_site_service.print_help()
        elif args.subcommand == 'storage':
            if args.subsubcommand == 'get':
                output['raw'] = SiteAPI(session=session).get_storage(storage_id=args.id)

                # make a nice table
                field_names = [
                    "Parent Node", "Parent Site", "Name", "ID",
                    "Host", "Base Path", "SRM", "Device Type", "Size (TB)",
                    "Protocols (Prefix / Port)"
                ]

                protocols = output["raw"].get('supported_protocols', [])
                protocol_info = "\n".join("{} / {}".format(
                    p.get('prefix', '-'), p.get('port', '-')
                ) for p in protocols)

                row = [
                    output["raw"].get("parent_node_name", "-"),
                    output["raw"].get("parent_site_name", "-"),
                    output["raw"].get("name", "-"),
                    output["raw"].get("id", "-"),
                    output["raw"].get("host", "-"),
                    output["raw"].get("base_path", "-"),
                    output["raw"].get("srm", "-"),
                    output["raw"].get("device_type", "-"),
                    output["raw"].get("size_in_terabytes", "-"),
                    protocol_info
                ]

                # Optionally show storage area info
                if args.areas:
                    field_names.append("Areas (ID / Name)")
                    areas = output["raw"].get('areas', [])
                    area_info = "\n".join("{} / {}".format(
                        area.get('id', '-'), area.get('name', '-')
                    ) for area in areas)
                    row.append(area_info)

                table = PrettyTable()
                table.field_names = field_names
                table.align = 'l'
                table.add_row(row)

                output["tabulated"] = table
            elif args.subsubcommand == "ls":
                output["raw"] = SiteAPI(session=session).list_storages(
                    node_name=args.node,
                    site_name=args.site,
                )

                # make a nice table
                table = PrettyTable()
                table.field_names = [
                    "Parent Node", "Parent Site", "Name", "ID",
                    "Host", "Base Path", "SRM", "Device Type", "Size (TB)"
                ]
                table.align = 'l'

                for storage in output['raw']:
                    table.add_row([
                        storage.get('parent_node_name', '-'),
                        storage.get('parent_site_name', '-'),
                        storage.get('name', '-'),
                        storage.get('id', '-'),
                        storage.get('host', '-'),
                        storage.get('base_path', '-'),
                        storage.get('srm', '-'),
                        storage.get('device_type', '-'),
                        storage.get('size_in_terabytes', '-')
                    ])
                output['tabulated'] = table
            else:
                parsers.srcnet_oper_site_storage.print_help()
        elif args.subcommand == 'storage-area':
            if args.subsubcommand == 'get':
                output['raw'] = SiteAPI(session=session).get_storage_area(storage_area_id=args.id)

                # make a nice table
                table = PrettyTable()
                table.field_names = [
                    "Parent Node", "Parent Site", "Name", "ID",
                    "Parent Storage ID", "Type", "Relative Path", "Tier"
                ]
                table.align = 'l'

                table.add_row([
                    output["raw"].get('parent_node_name', '-'),
                    output["raw"].get('parent_site_name', '-'),
                    output["raw"].get('name', '-'),
                    output["raw"].get('id', '-'),
                    output["raw"].get('parent_storage_id', '-'),
                    output["raw"].get('type', '-'),
                    output["raw"].get('relative_path', '-'),
                    output["raw"].get('tier', '-')
                ])
                output['tabulated'] = table
            elif args.subsubcommand == 'ls':
                output['raw'] = SiteAPI(session=session).list_storage_areas(
                    node_name=args.node,
                    site_name=args.site,
                )

                # make a nice table
                table = PrettyTable()
                table.field_names = [
                    "Parent Node", "Parent Site", "Name", "ID",
                    "Type", "Relative Path", "Tier"
                ]
                table.align = 'l'

                for entry in output['raw']:
                    table.add_row([
                        entry.get('parent_node_name', '-'),
                        entry.get('parent_site_name', '-'),
                        entry.get('name', '-'),
                        entry.get('id', '-'),
                        entry.get('type', '-'),
                        entry.get('relative_path', '-'),
                        entry.get('tier', '-')
                    ])
                output['tabulated'] = table
            else:
                parsers.srcnet_oper_site_storage_area.print_help()
        else:
            parsers.srcnet_oper_site.print_help()
    elif args.command == 'token':
        if args.subcommand == 'exchange':
            session.exchange_token(service_name=args.service, version="latest")
        elif args.subcommand == 'get':
            access_token = session.get_access_token(service_name=args.service)
            if access_token:
                output['raw'] = access_token
            else:
                output['raw'] = "Token for service {} not found.".format(args.service)
        elif args.subcommand == 'ls':
            output['raw'] = session.list_access_tokens()

            # make a nice table
            table = PrettyTable()
            table.field_names = ["Service Name", "Access token", "Expires at (UTC)", "Expires at (Local)",
                                 "Path on disk", "Has associated refresh token?"]
            table.align = 'l'
            table.align["Has associated refresh token?"] = 'c'

            for aud, attributes in output['raw'].items():
                table.add_row([
                    aud,
                    attributes.get('access_token')[0:20],
                    datetime.datetime.fromtimestamp(attributes.get('expires_at'), datetime.timezone.utc),
                    datetime.datetime.fromtimestamp(attributes.get('expires_at')),
                    attributes.get('path_on_disk'),
                    u'\u2713' if attributes.get('has_associated_refresh_token') else ''
                ])
            output['tabulated'] = table
        elif args.subcommand == 'inspect':
            output['raw'] = session.inspect_access_token(service_name=args.service)
        elif args.subcommand == 'request':
            session.start_device_flow()
        else:
            parsers.srcnet_oper_token.print_help()
    else:
        parsers.srcnet_oper.print_help()

    if args.json:
        if output.get('raw'):
            print(json.dumps(output.get('raw')))
    else:
        if output.get('tabulated'):
            print(output.get('tabulated'))
        elif output.get('raw'):
            pprint.pprint(output.get('raw'))
